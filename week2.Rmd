---
title: "Week 2 Assignment"
author: "Alejandro Borges Sanchez"
date: "1/17/2022"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(dtplyr)
library(doParallel)
registerDoParallel(cores=6)
raw_data_dir<-paste0(here::here(),"/../capstone raw data/")
en_data_dir<-paste0(raw_data_dir,"final/en_US")
board_folder=glue::glue("{Sys.getenv('PIN_LOCAL_FOLDER')}capstone")
board <- pins::board_folder(board_folder,versioned = TRUE)
## Helper functions for down the road

write_sampled_and_tokenized <- function() {
  pins::pin_write(board = board,
                  x = sampled_files,
                  name = "sampled_files",
                  description = "Coursera capstone 33% sampled files",
                  type="arrow")

pins::pin_write(board = board,
                x = tok_corpora %>% as_tibble(),
                name = "tok_corpora",
                description = "Coursera capstone tokenized no stopwords",
                type="arrow")


pins::pin_write(board = board,
                x = tok_corpora_bigram%>% as_tibble(),
                name = "tok_corpora_bigram",
                description = "Coursera capstone bigram tokenized no stopwords",
                type="arrow")

pins::pin_write(board = board,
                x = tok_corpora_trigram%>% as_tibble(),
                name = "tok_corpora_trigram",
                description = "Coursera capstone trigram tokenized no stopwords",
                type="arrow")
 
pins::pin_write(board = board,
                x = tok_corpora_freq %>% as_tibble(),
                name = "tok_corpora_freq",
                description = "Coursera capstone tokenized no stopwords",
                type="arrow")

pins::pin_write(board = board,
                  x = tok_corpora_bigram_freq,
                  name = "tok_corpora_bigram_freq",
                  type="arrow" )

pins::pin_write(board = board,
                  x = tok_corpora_trigram_freq,
                  name = "tok_corpora_trigram_freq",
                  type="arrow" )

pins::pin_write(board = board,
                x = tok_corpora_freq_idf %>% as_tibble(),
                name = "tok_corpora_freq_idf",
                description = "Coursera capstone tokenized no stopwords",
                type="arrow")

pins::pin_write(board = board,
                  x = tok_corpora_bigram_freq_idf,
                  name = "tok_corpora_bigram_freq_idf",
                  type="arrow" )

pins::pin_write(board = board,
                  x = tok_corpora_trigram_freq_idf,
                  name = "tok_corpora_trigram_freq_idf",
                  type="arrow" )
}

read_sampled_and_tokenized <- function() {
  
  sampled_files <<- pins::pin_read(board = board,
                                   name = "sampled_files")
  tok_corpora <<- pins::pin_read(board = board,
                                   name = "tok_corpora")
  tok_corpora_bigram <<- pins::pin_read(board = board,
                                   name = "tok_corpora_bigram")
 tok_corpora_trigram <<- pins::pin_read(board = board,
                                   name = "tok_corpora_trigram")
 
 tok_corpora_freq <<- pins::pin_read(board = board,
                                   name = "tok_corpora_freq")
 tok_corpora_bigram_freq <<- pins::pin_read(board = board,
                                   name ="tok_corpora_bigram_freq")
 tok_corpora_trigram_freq <<- pins::pin_read(board = board,
                                   name = "tok_corpora_trigram_freq")
 
 
 tok_corpora_freq_idf <<- pins::pin_read(board = board,
                                   name = "tok_corpora_freq_idf")
 tok_corpora_bigram_freq_idf <<- pins::pin_read(board = board,
                                   name ="tok_corpora_bigram_freq_idf")
 tok_corpora_trigram_freq_idf <<- pins::pin_read(board = board,
                                   name = "tok_corpora_trigram_freq_idf")
}

calc_frequency <- function(data,colname){
  colname_freq <- paste0(colname,"_freq")
  colname_total <- glue::glue("total_{colname}")
  colname_count <- glue::glue("{colname}_count")
  data %>% 
    select(.data[[colname]]) %>% 
    add_count(sort = TRUE,name = colname_total) %>% 
    add_count(.data[[colname]],name=colname_count,sort=TRUE) %>%
    distinct() %>% 
    mutate(
        {{colname_freq}} := .data[[colname_count]]/.data[[colname_total]]
    )
}


clean_text <- function(data,colname="text"){
  data %>% 
  filter(!str_detect(.data[[colname]],"^[0-9]+$|^rt$|^lol$|^_+$|^[0-9\\._]{2,}$|class=\\.*|style=\\.*")) %>% 
    mutate(text=str_remove(text,"Ã¸"))
}

read_sampled_and_tokenized_m <-  memoise::memoise(read_sampled_and_tokenized)
```

```{r include=FALSE,cache=TRUE}
read_sampled_and_tokenized_m()
```

```{r include=FALSE, cache=TRUE}
wcout<- system(glue::glue("wc ./../capstone\\ raw\\ data/final/*/*"),intern=TRUE)

basic_file_stats <- tibble( line=wcout) %>% 
    extract(
        line,
        into=c("nl_count","word_count","byte_count","file_name"),
        regex = regex("^ +([0-9]+) +([0-9]+) +([0-9]+).+\\/([^\\/]+)$",ignore_case = TRUE)
    )
```

```{r include=FALSE}
random_chunk_reader<-function(x,pos){
    tibble(
        text=sample(x,length(x)*0.33)
    )
}
sampled_files<-list.files(en_data_dir)%>%
    purrr::map_df(.,function(fname){
        tibble(file=fname,
               content=list(readr::read_lines_chunked(paste0(en_data_dir,"/",fname),
                                  callback = DataFrameCallback$new(random_chunk_reader))
               ))
    })%>%
  unnest(cols=c(content))%>%
  group_by(file)%>%
  mutate(rnum=row_number())%>%
  ungroup()%>%
   mutate(
        from=str_remove(file,"en_US\\."),
        from=str_remove(from,"\\.txt")
    )
```

## The data

I loaded a 33% normally drawn sample of each file as my exploration dataset. I intend to split and train further down the line when modeling. A 33% random sample stratified by file (blogs, twitter, news) should be more than enough for exploration.

```{r}

basic_file_stats %>% 
  mutate(across(1:3,~ parse_number(.x))) %>%
  mutate(across(1:3,~ scales::label_comma()(.x))) %>%
  rename(lines=1,words=2,bytes=3) %>% 
  knitr::kable()
```

We are only interested in english, but thats what is included in our corpus.

## Cleaning

There is a lot to clean in the text. Blog text data can and does include html code that is useless to us. Twitter is a culture of hashtags and codewords due to its character limits, and some of those need to be excluded.

This is an iterative process that would be too long to document here, the code of the clean_text function is available in the source code for this RMD file for reproducibility purposes.

## Exploratory Analysis

After cleaning the basic text, I parsed it in three distinct datasets.

-   tok_corpora: a 1 word per row token dataset

-   tok_corpora_bigram: same as above, for bigrams

-   tok_corpora_trigram: same, but for three words

Once we have this kind of formating, we can start showing you the basics of what this data set contains. Calculations are in order to tokenize and thus from this three base datasets, others are produced: some have frequencies, some have other text mining measures like TF-IDF. All of this is available in the source code. Once that is done, we get basic frequency analysis for one, bi and trigrams.

### Distribution of ngram frequencies

#### one-grams

```{r}
tok_corpora_freq %>% 
    ggplot(aes(x=word_freq,fill=from))+ 
    geom_histogram()+
    facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(title = "Word frequency distribution by source of text")
```

#### bigrams

```{r}
tok_corpora_bigram_freq %>% 
  ggplot(aes(x=bigram_freq,fill=from)) +
  geom_histogram() +
  facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+
  labs(title = "Bigram frequency distribution by source of text")


```

#### trigrams

```{r}
tok_corpora_trigram_freq %>% 
  ggplot(aes(x=trigram_freq,fill=from)) +
  geom_histogram() +
  facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+
  labs(title = "Trigram frequency distribution by source of text")
```

### Top 20 most frequent words

#### For one-grams 

```{r}
tok_corpora_freq %>% 
    filter(!str_detect(word,"[0-9 ]")) %>% 
    arrange(desc(word_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(word=tidytext::reorder_within(word,desc(word_freq),from)) %>% 
    ggplot(aes(x=word_freq,y=word,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free")+
  coord_flip()+theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

#### For bigrams

```{r}
tok_corpora_bigram_freq %>% 
  filter(!str_detect(bigram,"NA NA|([0-9]+) ([0-9]+)")) %>% 
    arrange(desc(bigram_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(
        bigram=tidytext::reorder_within(bigram,desc(bigram_freq),from)
        )%>% 
    ggplot(aes(x=bigram_freq,y=bigram,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free")+
  coord_flip()+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

#### For trigrams

```{r}
tok_corpora_trigram_freq %>%
    filter(!str_detect(trigram,"^NA NA NA$|^amazon[\\. ]|([0-9]+) ([0-9]+) ([0-9]+)")) %>% 
    arrange(desc(trigram_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(
        trigram=tidytext::reorder_within(trigram,desc(trigram_freq),from)
        )%>% 
    ggplot(aes(x=trigram_freq,y=trigram,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free") + 
  coord_flip()+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

##  

### TF-IDF: a better frequency analysis

From the previous plots you can see that plain analysis frequency of n-grams (**or term frequency**) only gets us so far. In [the book](https://www.tidytextmining.com/) im using as a base for this project, the authors propose text frequency-inverse document frequency (TF-IDF) as a way to summarize a document by ngram frequency:

> The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

tf-idf is confusing since it looks like a substraction when in fact the statistic is calulated by multiplying the two quantities:

$$
tf\_idf=tf\ *\ idf
$$

Where **tf** is just term frequency: the number of times a term appears divided by the number of terms in a document (here our three "documents" are the texts extracted from: blogs, twitter and news). And **idf** is deemed a heuristic quantity useful for text mining but with some shaky information theory foundations. Since we are doing text mining here, we can just calculate it as such:

$$
idf(term) = ln(\frac{n_{documents}}{n_{documents\ containing\ term}})
$$

The {tidytext}[@silge2016] package im using calculates TF-IDF given a tidy dataset with at least two columns with: *term*,*term_count*, one row per term. As before, I direct you to the source code if you want to see how its done.

#### For one-grams

```{r}
tok_corpora_freq_idf %>%
    filter(!str_detect(word,"^amazon\\..*$")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(word=reorder_within(word,desc(tf_idf),from)) %>% 
    # ungroup() %>% 
    ggplot(aes(x=word,y=tf_idf,fill=from))+
    geom_col()+
    scale_x_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(fill=NULL)
```

#### For bigrams

```{r}

tok_corpora_bigram_freq_idf %>%
    filter(!str_detect(bigram,"^amazon[\\. ].*$")) %>%
  filter(!str_detect(bigram,"NA NA|([0-9]+) ([0-9]+)|rt rt|lol rt")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(bigram=reorder_within(bigram,desc(tf_idf),from)) %>% 
    ungroup() %>% 
    ggplot(aes(x=bigram,y=tf_idf,fill=from))+
    geom_col()+
    scale_x_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(fill=NULL)

```

#### For trigrams

```{r}
tok_corpora_trigram_freq_idf %>%
    filter(!str_detect(trigram,"^amazon[\\. ].*$|omg omg omg|cake cake cake|rt rt rt|ass ass ass|follow follow follow|^NA NA NA$|^amazon[\\. ]|([0-9]+) ([0-9]+) ([0-9]+)")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(trigram=reorder_within(trigram,desc(tf_idf),from)) %>% 
    ungroup() %>% 
    ggplot(aes(y=trigram,x=tf_idf,fill=from))+
    geom_col()+
    scale_y_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")+
  coord_flip()+
    labs(fill=NULL)
```

In all three of the above cases, we can see a very different list of the 20 top terms appears when using tf-idf compared to plain frequency analysis. By the way, tf-idf distribution has the same long tail of the distributions of term frequencies. This is to be expected in any language, as far as we know.
