---
title: "Targets pipeline"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

### The Coursera Solution Pipeline[^1]

[^1]: For reference the oficial manual is: <https://books.ropensci.org/targets/markdown.html#target-markdown>

Full documentation for the rationale behind this implementation is in the ENDGAME.Rmd file.

The diagram for the pipeline which will allow iteration from cleaning and onwards is [here](https://docs.google.com/drawings/d/1ulZlV0PZG9nbkvX4l6Qm6_dJ9GT1m37ahK7CU9PbaNE/edit?usp=sharing) :

![](https://docs.google.com/drawings/d/e/2PACX-1vR1KnEzjXiu1x1VSh-ut4uzriZuXYqkQK8C65Xrrc5I_LmWQ4rb0k03-K4SJ2Wpr5hIcQDhvE5LLA0q/pub?w=960&h=721){style="padding: 10px;" width="1066"}

### Implementing the pipeline

#### Setup

Basic startup setup

```{r}
library(targets) 
tar_unscript()
package_list <- c( "dplyr", "ggplot2", "readr", "tidyr","data.table","progressr","R6","pins","rsample","tidytext","purrr","furrr","magrittr","stringr","dtplyr")
xfun::pkg_attach(package_list)
```

```{targets t-globals, tar_globals=TRUE}
options(tidyverse.quiet = TRUE)
targets::tar_option_set(format = 'qs')
targets::tar_option_set(packages = c( "dplyr", "ggplot2", "readr", "tidyr","data.table","progressr","R6","pins","rsample","tidytext","purrr","furrr","magrittr","stringr","dtplyr"))
source("R/global_variables.R")
```

#### Raw data definition

Preclean is automatic: it lists the files in the source directory and pipes it to clean_files, which outputs the three files into an output directory

```{targets preclean-target}
source("R/preclean_functions.R")
source("R/kenlm_functions.R")
# If you want to actually load the text into R, youd do this.
# list(tar_target(preclean, 
#     preclean_files(
#     list.files(en_data_dir,pattern = '*.txt')),
#     format="file"
# ),
# tar_target(precleaned_text, 
#     load_clean_text(),
#     format="qs"
# ))
# 
# But we prefer generating the model with kenlm
# 

  list(tar_target(preclean,
             preclean_files(
               list.files(en_data_dir,pattern = '*.txt')
             ),
             format="file"
  ),
  tar_target(single_cleantext_file,
             create_single_cleantext_file(
               source_files = tar_read(preclean)
             ),
             format="file"
  ),
   tar_target(kenlm_arpa_file,
             create_kenlm_arpa(tar_read(single_cleantext_file)),
             format = 'file')
  )
 

```



```{r}
tar_make()
```

So so far we now have the precleaned_text as a target output. AS we are in an targets Rmd file, to access that object we need to use tar_read. If we were just using target scripts, that gets loaded by itself.

```{r}

tar_visnetwork()
```

The interesting part comes to see if we change any of the functions, will it reload and recreate the precleaned_text? I went and changed stuff in the preclean_files function and sure enough, it works.\


Many million rows...

```{targets summary-precleaned-text-target}
library(tidyverse)
library(dtplyr)
tar_target(
  preclean_summary,
 tar_read("precleaned_text")  %>%
  dtplyr::lazy_dt() %>% 
  group_by(file=as_factor(file)) %>% 
  as_tibble() %>% 
  mutate(avlength=str_count(text)) %>% 
  summary() 
)
```

Lets see this summary as a part of the pipeline:


```{r}
tar_make()
```


```{r}

tar_visnetwork()
```

```{r}
tar_read("preclean_summary")
```


#### Setencifying the text

The next target in the pipeline requires us to sentencify all the text. That is, to turn everything into sentences which will yield the actual corpus object we are interested in as is depicted in the diagram. 

```{targets sentecify-target}
source("R/sentence_tokenizer_functions.R")
# tar_target(
#   corpus,
#   tar_read("precleaned_text") %>% sentencify()
# )

```


```{r}
tar_make()
```


```{r}

tar_visnetwork()
```
