---
title: "Targets pipeline"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

### The Coursera Solution Pipeline[^1]

[^1]: For reference the oficial manual is: <https://books.ropensci.org/targets/markdown.html#target-markdown>

Full documentation for the rationale behind this implementation is in the ENDGAME.Rmd file.

The diagram for the pipeline which will allow iteration from cleaning and onwards is [here](https://docs.google.com/drawings/d/1ulZlV0PZG9nbkvX4l6Qm6_dJ9GT1m37ahK7CU9PbaNE/edit?usp=sharing) :

![](https://docs.google.com/drawings/d/e/2PACX-1vR1KnEzjXiu1x1VSh-ut4uzriZuXYqkQK8C65Xrrc5I_LmWQ4rb0k03-K4SJ2Wpr5hIcQDhvE5LLA0q/pub?w=960&h=720){style="padding: 10px;" width="1066"}

### Implementing the pipeline

#### Setup

Basic startup setup

```{r setting-up}
library(targets) 
tar_unscript()
package_list <-
  c(
    "dplyr",
    "ggplot2",
    "readr",
    "tidyr",
    "data.table",
    "progressr",
    "R6",
    "pins",
    "rsample",
    "tidytext",
    "purrr",
    "furrr",
    "magrittr",
    "stringr",
    "dtplyr",
    "targets",
    "tarchetypes"
  )
xfun::pkg_attach2(package_list)
```

```{targets t-globals, tar_globals=TRUE}
package_list <-
  c(
    "dplyr",
    "ggplot2",
    "readr",
    "tidyr",
    "data.table",
    "progressr",
    "R6",
    "pins",
    "rsample",
    "tidytext",
    "purrr",
    "furrr",
    "magrittr",
    "stringr",
    "dtplyr",
    "futile.logger"
  )
# options(tidyverse.quiet = TRUE)
targets::tar_option_set(format = 'qs')
targets::tar_option_set(packages = package_list)
library(targets)
xfun::pkg_attach(package_list)
source("R/global_variables.R")
options(datatable.verbose = FALSE)

```

#### Building an iteration friendly environment

As is evident from the pipeline, text comes with nulls and all sorts of nasty. This requires a precleaning step:

```{targets preclean-target}
source("R/preclean_functions.R")

  tar_target(preclean,
             preclean_files(
               list.files(en_data_dir, pattern = '*.txt')
             ))
```

After that and crucially prior to sampling, we "sentencify", that is, we split the text into sentences:

```{targets sentencify-target}
source("R/sentencify.R")

  tar_target(
    sentenced_files,
    sentencify(precleaned_files = preclean),
    
    format = "file"
  )
```

Now here comes an interesting part. I do a quick sample of sentences of the whole corpus. Only loading 1% of it:

```{targets sample-one-pct-target}
source("R/simple_sample.R")
source("R/load_text_as_table.R")
tar_target(sample_one_pct,
           simple_sample(sentenced_files,pct = 0.01),
           format = "file")
  
```

Having sampled, we can now split in training and testing set:

```{targets splits_of_onepct}
source("R/create_our_splits.R")
tar_target(splits_of_onepct,
           create_our_splits(sample_one_pct))
```

And from that, a subsplit of training and the final training set, given back as lists. Those need to be extracted into separate targets just for clean tar_target() code down the pipeline:

```{targets training-devtest-testing}
source("R/create_training_and_devset.R")

list(
  tar_target(
    testing_onepct,
    splits_of_onepct %>% 
      rsample::testing()
    ),
    tar_target(pre_training_onepct,
               splits_of_onepct %>%
                 rsample::training()
               ),
    tar_target(
      training_devtest_split_onepct,
      create_training_and_devset(pre_training_onepct)
    ),
    tar_target(training_onepct,
               purrr::pluck(training_devtest_split_onepct, "training")
               ),
    tar_target(devtest_onepct,
               purrr::pluck(training_devtest_split_onepct, "devtest")
               )
    
  )
  

```

Ok, now we are cleaning the text by sentence so we can later down the line finally build models and then evaluate them.

We will try to do this in parallel as per <https://books.ropensci.org/targets/hpc.html#future-locally>, so first we run all those above sequentially:

```{r first-make}
tar_make()
```

And now we build the parallel cleaning targets:

```{targets cleaning-target}

source("R/clean_texts.R")
list(
  tar_target(
    clean_testing_onepct,
    clean_texts_dt(testing_onepct)
  ),
  tar_target(
    clean_training_onepct,
    clean_texts_dt(training_onepct)
  ),
  tar_target(
    clean_devtest_onepct,
    clean_texts_dt(devtest_onepct)
  )
  
)

```

And build those...

```{r paralel-make-cleaning}
library(future)
library(future.callr)
plan(callr)
tar_make_future(workers = 6)
```

Now we have all we need toooo....

#### Building models

We build the model exclusively on the training set. This is how to do that:

```{targets training-file-as-onepct-target}
source("R/kenlm_functions.R")

# KenLM requires a simple clean text file to build. 
# So we do that with our training set

  tar_target(training_as_file_onepct,
             create_file_from_data_table_set(
               clean_training_onepct,
               'training_corpus_onepct.txt'
             ),
             format="file")
             
```

```{targets kenlm-arpafile-onepct-target}

tar_target(kenlm_arpafile_onepct,
             create_kenlm_arpa(
               source_file = tar_read(training_as_file_onepct),
               outfile = 'kenlm_model_onepct'
                               ),
                               
             format='file')

```

```{targets kenlm-model-onepct-target}

  tar_target(kenlm_model_onepct,
          load_arpa_as_data_table(
            source_file = kenlm_arpafile_onepct
              ))

```


And then make those...

```{r kenlm-make}
 tar_make()
```

Now, modeling actually has a cutoff value, that is, a lot of monograms are not even allowed to be in the model. Additionally, it is just plain normal that the test and devtest sets have words that are unks.

Unks are not only hard to search for but also have to be replaced throughout all texts. This means its better to preprocess them.

```{targets ngramified-devtest-onepct-target}
source('./R/model_functions.R')
tar_target(ngramified_devtest_onepct,
tar_read(clean_devtest_onepct) %>% 
    n_gramify_all(text_dt = .,
                        model_dt = tar_read(kenlm_model_onepct))

)

```

```{r ngramified-devtest-onepct-make}
tar_make()
```

Unkified:

```{targets unkified-devtest-onepct-target}
source('./R/model_functions.R')
tar_target(
  unkified_devtest_onepct,
  tar_read(ngramified_devtest_onepct) %>%
    unkify_all(
      unked_text_dt = .,
      model_dt = tar_read(kenlm_model_onepct),
      tok_unk='<unk>'
    )
  
)

```

```{r unkified-devtest-onepct-make}
tar_make()
```


#### Model evaluation

So now we can actually evaluate the devtest:

```{targets sentenced-devtest-onepct-target}
source('./R/model_functions.R')

tar_target(
  devtest_onepct_sentences_probs,
  tar_read(unkified_devtest_onepct) %>%
    corpus_probabilities_by_sentence(model_dt = tar_read(kenlm_model_onepct))
) 

```

```{r sentenced-devtest-onepct-make}
tar_make()
```



As for the evaluation, we now have one model and one devtest and the probability of that devtest with backoff values and not. This can be seen here, as we evaluate the held-out devset:

```{r corpus-probability-table}
source('R/model_functions.R')
tar_read(devtest_onepct_sentences_probs) %>% 
  corpus_probability()
```

This is expressed on the log scale. We can see it for a sample of the training set here and it should be way, way higher, as the model was generated from it:

```{targets ngramified-training-onepct-target}
source('./R/model_functions.R')
  tar_target(
    ngramified_training_onepct,
    tar_read(clean_training_onepct) %>%
      n_gramify_all(text_dt = .,
                    model_dt = tar_read(kenlm_model_onepct))
    
  )
```


```{r ngramified-training-onepct-make}
tar_make()
```

```{targets ngramified-unkified-training-onepct-target}
tar_target(
    unkified_training_onepct,
    tar_read(ngramified_training_onepct) %>%
      unkify_all(
        unked_text_dt = .,
        model_dt = tar_read(kenlm_model_onepct)
      )
    
  )
```


```{r ngramified-unkified-training-onepct-make}
tar_make()
```


```{targets training-sentences-probs-target}
tar_target(
    training_onepct_sentences_probs,
    tar_read(unkified_training_onepct) %>%
      corpus_probabilities_by_sentence(model_dt = tar_read(kenlm_model_onepct))
  )
```

```{r training-sentences-probs-make}
tar_make()
```

##### Probability of corpuses


The probablity of a corpus is given by the straight sum of the log odds of each sentence. What follows is the training and devtest densities of the total probability (log odds + backoff) 

```{r}
 list(
     tar_read(training_onepct_sentences_probs) %>% .[,its:="training"],
     tar_read(devtest_onepct_sentences_probs) %>% .[,its:="devtest"]) %>% 
     rbindlist() %>% 
     ggplot(aes(x=t,color=its))+
     geom_density()
```

Bearing in mind that this kenlm model is built with 40 as cutoff value, that does not look too crazy, but it does not look too efficient.

And this is what this pretty pipeline, from precleaning to generating a model, looks like:

```{r show-off-network}
tar_visnetwork()
```

This is some sort of childish because kenlm is really much more efficient in evaluating and calculating perplexity on whole corpuses. So we have another way to construct the pipeline that takes advantage of this fact. 

Here it goes.

### The KenLM way

In the kenLM way, we might preserve trhe strategy all the way to splits


#### Tunning
Ok, of to the subject of more close up analisys of this thing, we must see really what differs. For one, I want to see how does cutoff value change, and what happens if we deal with lowercase only data. 

```{r}
expand.grid(
    to_lower=c(TRUE,FALSE),
    kenlm_cutoff=c(10,20,30)
)
```
