---
title: "YARGODYNE Predictive Keyboard App"
author: "Alejandro Borges Sanchez"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(targets)
knitr::opts_chunk$set(echo = FALSE)
```

## The Algorithm

-   Top of the line high performance algorithm Kneser-Ney n-gram model as implemented for high performance by [Keneth Heafield](https://kheafield.com/code/kenlm), described in his [seminal paper](https://kheafield.com/papers/edinburgh/estimate_paper.pdf) "Scalable Modified Kneser-Ney Language Model Estimation".

-   It allows us to estimate the language model through pruning and compute perplexity on large combinations of parameters, thus yielding a small model yet with powerful estimation capabilities.

-   R interfaces for language model estimation and evaluation were built to automate the process in a reproducible r {targets} pipeline.

## Best model

-   The main constraint is model size, which is kept under 400mb.
-   Searching and finding the best model under this constraints was possible due to our algorithm implementation choice.

We searched through:

-   A grid of 1792 models:
    -   Of sample sizes from 10% to 90% of the language
    -   lowercase and uppercase
    -   Ngram orders ranging from 1 to 6
    -   Pruning ranging from 0 to 40

## Model selection 

```{r}
tar_read(consolidated_evaluations) %>%  
    rename(model_size=size) %>% 
    select(-model_file,-evaluated_on) %>% 
    select(1:5,model_size) %>% 
    filter(model_size<=fs::as_fs_bytes("400M")) %>% 
    group_by(case,order,prune,sample_size) %>% 
    filter(perplexity_including_oo_vs==min(perplexity_including_oo_vs)) %>% 
    group_by(sample_size) %>% 
    filter(perplexity_including_oo_vs==min(perplexity_including_oo_vs)) %>% 
    ungroup() %>% 
    arrange(perplexity_including_oo_vs) %>% 
  mutate(across(where(is.numeric),~round(.x,digits=2))) %>% 
  rename(
         perplexity=perplexity_including_oo_vs) %>% 
    knitr::kable(
      digits =2,
      row.names = FALSE,
    align='l',
      caption = 'We select the lowest perplexity models under 400MB for each letter case, ngram order, prune size and sample size combination. Then, out of those, we select the lowest perplexity models of each sample size.'
    )
```

## The best model

It seems we can get close to a 400MB size model with the best perplexity using almost the whole corpus (90%) lower cased, building a 4-gram language model, pruning all singletons (prune=1) of all orders.

## The app

-   You can access the app here.

-   To use it, simply introduce your text and a 1 word completion should appear, representing the most probable ending to the phrase.

-   An animated graph interface shows the algorithm traversing the language model in real time.
