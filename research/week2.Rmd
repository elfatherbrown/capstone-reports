---
title: "Week 2 Assignment"
author: "Alejandro Borges Sanchez"
date: "1/17/2022"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
source("globals.R")
```

```{r include=FALSE,cache=TRUE}
read_sampled_and_tokenized_m()
```

```{r include=FALSE, cache=TRUE}
wcout<- system(glue::glue("wc ./../capstone\\ raw\\ data/final/*/*"),intern=TRUE)

basic_file_stats <- tibble( line=wcout) %>% 
    extract(
        line,
        into=c("nl_count","word_count","byte_count","file_name"),
        regex = regex("^ +([0-9]+) +([0-9]+) +([0-9]+).+\\/([^\\/]+)$",ignore_case = TRUE)
    )
```

```{r include=FALSE}
random_chunk_reader<-function(x,pos){
    tibble(
        text=sample(x,length(x)*0.33)
    )
}
sampled_files<-list.files(en_data_dir)%>%
    purrr::map_df(.,function(fname){
        tibble(file=fname,
               content=list(readr::read_lines_chunked(paste0(en_data_dir,"/",fname),
                                  callback = DataFrameCallback$new(random_chunk_reader))
               ))
    })%>%
  unnest(cols=c(content))%>%
  group_by(file)%>%
  mutate(rnum=row_number())%>%
  ungroup()%>%
   mutate(
        from=str_remove(file,"en_US\\."),
        from=str_remove(from,"\\.txt")
    )
```

## Introduction

This document was written as a status report to a manager. As such

## The data

Files in several languages from three sources where provided:

-   Sources

    -   Twitter

    -   News

    -   Blogs

-   Languages

    -   Deutsch

    -   Finish

    -   Russian

However, we limit ourselves to english

```{r include=TRUE}

basic_file_stats %>% 
  mutate(across(1:3,~ parse_number(.x))) %>%
  mutate(across(1:3,~ scales::label_comma()(.x))) %>%
  rename(lines=1,words=2,bytes=3) %>% 
  filter(str_detect(file_name,"en_")) %>% 
  knitr::kable()
```

Attempting to do full analysis of this corpus would be ill advised due to memory and cpu constraints. I sampled 33% of each source and analyzed them separately to render this exercise.

## Cleaning

There is a lot to clean in the text. Blog text data can and does include html code that is useless to us. Twitter is a culture of hashtags and codewords due to its character limits, and some of those need to be excluded.

This is an iterative process that would be too long to document here.

## Exploratory Analysis

After cleaning the basic text, I parsed it in three distinct datasets.

-   tok_corpora: a 1 word per row token dataset

-   tok_corpora_bigram: same as above, for two words per token

-   tok_corpora_trigram: same, but for three words

Once we have this kind of data format, we can start showing the basics of what this data set contains.

### Distribution of ngram frequencies

A first approach to check the sanity of the data sets is looking at the term frequency. Languages have a small set of words that get repeated a lot and a lot of words that are not as frequent. We would thus expect frequencies to have long tails. If we didnt, the data would be immediately suspect.

#### one-grams

```{r}
tok_corpora_freq %>% 
    ggplot(aes(x=word_freq,fill=from))+ 
    geom_histogram()+
    facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(title = "Word frequency distribution by source of text")
```

#### bigrams

```{r}
tok_corpora_bigram_freq %>% 
  ggplot(aes(x=bigram_freq,fill=from)) +
  geom_histogram() +
  facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+
  labs(title = "Bigram frequency distribution by source of text")


```

#### trigrams

```{r}
tok_corpora_trigram_freq %>% 
   filter(!str_detect(trigram,"^amazon[\\. ].*$|omg omg omg|cake cake cake|rt rt rt|ass ass ass|follow follow follow|^NA NA NA$|^amazon[\\. ]|([0-9]+) ([0-9]+) ([0-9]+)")) %>%
  ggplot(aes(x=trigram_freq,fill=from)) +
  geom_histogram() +
  facet_wrap(
        . ~ from,scales = "free"
    )+
    xlim(NA,0.000009)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+
  labs(title = "Trigram frequency distribution by source of text")
```

All of this distributions look like they should: high at the most frequent and a long tail to the right.

### Top 20 most frequent terms

Now we can start looking at the most frequent terms for one, two and three worded terms:

#### For one-grams 

```{r}
tok_corpora_freq %>% 
    filter(!str_detect(word,"[0-9 ]")) %>% 
    arrange(desc(word_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(word=tidytext::reorder_within(word,desc(word_freq),from)) %>% 
    ggplot(aes(x=word_freq,y=word,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free")+
  coord_flip()+theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

Love, day, people, time stand out in all three sources of text. It is interesting even at this early stage how some words are more "news"y (percent, million, county), others more "blog"y (god, family, books), others well... yup, very twitteresque (rt, lol, im, game)...

#### For bigrams

```{r}
tok_corpora_bigram_freq %>% 
  filter(!str_detect(bigram,"NA NA|([0-9]+) ([0-9]+)")) %>% 
    arrange(desc(bigram_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(
        bigram=tidytext::reorder_within(bigram,desc(bigram_freq),from)
        )%>% 
    ggplot(aes(x=bigram_freq,y=bigram,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free")+
  coord_flip()+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

When we do it by bigrams, then some meaning starts to emerge. Names of cities, government positions in the news, salutations in twitter, everyday interests in blogs.\

#### For trigrams

```{r}
tok_corpora_trigram_freq %>%
   filter(!str_detect(trigram,"^amazon[\\. ].*$|omg omg omg|cake cake cake|rt rt rt|ass ass ass|follow follow follow|^NA NA NA$|^amazon[\\. ]|([0-9]+) ([0-9]+) ([0-9]+)")) %>%
    arrange(desc(trigram_freq)) %>% 
    group_by(from) %>% 
    slice_head(n = 20) %>%
    mutate(
        trigram=tidytext::reorder_within(trigram,desc(trigram_freq),from)
        )%>% 
    ggplot(aes(x=trigram_freq,y=trigram,fill=from)) +
    geom_col()+
    tidytext::scale_y_reordered()+
    facet_wrap(~from,ncol=1,scales="free") + 
  coord_flip()+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")
```

Trigrams continue offering more meaning than bigrams, and now its a more interesting characterization of what people talk about in these three text sources.

### TF-IDF: a better frequency analysis

From the previous plots you can see that plain analysis frequency of n-grams (**or term frequency**) only gets us so far. In [the book Tidy Text Mining](https://www.tidytextmining.com/) @silge2016, the authors propose term frequency-inverse document frequency (TF-IDF) as a way to summarize a document by ngram frequency in such a way that the most commonly used words weigh less than the lesser used. This yields a ranked list of terms characteristic of each document. From @silge2016:

> The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

The term *tf-idf* is confusing because it looks like a substraction when in fact the statistic is calulated by multiplying two quantities:

$$
tf\_idf=tf\ *\ idf
$$

Where **tf** is just term frequency: the number of times a term appears divided by the number of terms in a document (here our three "documents" are the texts extracted from: blogs, twitter and news). And **idf** is deemed a heuristic quantity useful for text mining but with some shaky information theory foundations. Since we are doing text mining here, we can just calculate it as such:

$$
idf(term) = ln(\frac{n_{documents}}{n_{documents\ containing\ term}})
$$

So lets look at the most important terms for each text source, in one, two and three worded grams:

#### For one-grams

```{r}
tok_corpora_freq_idf %>%
    filter(!str_detect(word,"^amazon\\..*$")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(word=reorder_within(word,desc(tf_idf),from)) %>% 
    # ungroup() %>% 
    ggplot(aes(x=word,y=tf_idf,fill=from))+
    geom_col()+
    scale_x_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(fill=NULL)
```

#### For bigrams

```{r}

tok_corpora_bigram_freq_idf %>%
    filter(!str_detect(bigram,"^amazon[\\. ].*$")) %>%
  filter(!str_detect(bigram,"NA NA|([0-9]+) ([0-9]+)|rt rt|lol rt")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(bigram=reorder_within(bigram,desc(tf_idf),from)) %>% 
    ungroup() %>% 
    ggplot(aes(x=bigram,y=tf_idf,fill=from))+
    geom_col()+
    scale_x_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
    theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0))+labs(fill=NULL)

```

#### For trigrams

```{r}
tok_corpora_trigram_freq_idf %>%
    filter(!str_detect(trigram,"^amazon[\\. ].*$|omg omg omg|cake cake cake|rt rt rt|ass ass ass|follow follow follow|^NA NA NA$|^amazon[\\. ]|([0-9]+) ([0-9]+) ([0-9]+)")) %>% 
    group_by(from) %>% 
    slice_max(tf_idf,n = 20) %>%
    mutate(trigram=reorder_within(trigram,desc(tf_idf),from)) %>% 
    ungroup() %>% 
    ggplot(aes(y=trigram,x=tf_idf,fill=from))+
    geom_col()+
    scale_y_reordered()+
    facet_wrap(~ from,scales = "free",ncol = 1)+
  theme(axis.text.x = element_text(angle = -30, vjust = 1, hjust = 0),legend.position = "none")+
  coord_flip()+
    labs(fill=NULL)
```

In all three of the above cases, we can see a very different list of the 20 top terms appears when using tf-idf compared to plain frequency analysis. The top 20 tf-idf terms is a much better way to get a gist of what is in the news, blogs and twitter since it eliminates terms that have too much frequency within each source document.

By the way, tf-idf distribution has the same long tail of the distributions of term frequencies. This is to be expected in any language, as far as we know, thus lending to the idea that our sources are somewhat well cleaned and actually have mostly English text in them.

## Conclusion

With this exploration, I believe we are ready to do some analysis.
